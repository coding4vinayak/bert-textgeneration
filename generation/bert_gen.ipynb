{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.11.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDXYy7OJtkWW",
        "outputId": "a65464b6-d0b1-4f94-ead3-072892b25db7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.11.3\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (1.22.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (4.65.0)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (6.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.11.3) (2022.12.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.11.3) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.11.3) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.11.3) (1.1.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=05bc36062279d7a58b93536ca9af9d37cbc617014e12df61c1682b25217b8c21\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the libraries"
      ],
      "metadata": {
        "id": "h4bbGFwwTp6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch "
      ],
      "metadata": {
        "id": "zJ-DsOygTrsA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkzysUjN0YEd",
        "outputId": "0c2f92c7-7e3f-46b4-9341-88048c7e5698"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Define the input text and tokenize it\n",
        "input_text = \"Le chat est [MASK] sur le canapé.\"\n",
        "tokenized_text = tokenizer.tokenize(input_text)\n",
        "\n",
        "# Find the index of the masked token\n",
        "masked_index = tokenized_text.index('[MASK]')\n",
        "\n",
        "# Convert the tokenized text to a tensor of token ids\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Move the tokens tensor to the CPU\n",
        "tokens_tensor = tokens_tensor.to(device)\n",
        "\n",
        "# Generate predictions for the masked token using the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0][0, masked_index].topk(5)\n",
        "\n",
        "# Convert the predicted token ids to tokens\n",
        "predicted_token_ids = predictions.indices.tolist()\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "\n",
        "# Print the predicted tokens\n",
        "print(predicted_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zodL4D2AzpBe",
        "outputId": "175ada9f-14d4-4477-aa55-ecce43ce18f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['représenté', 'présent', 'situé', 'porté', 'placé']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Define the input text\n",
        "input_text = \"Le chat noir est sur le canapé. Le chat blanc est sur le tapis.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokenized_text = tokenizer.tokenize(input_text)\n",
        "\n",
        "# Loop through each token in the input text\n",
        "for i in range(len(tokenized_text)):\n",
        "    # If the token is a mask, replace it with the predicted token\n",
        "    if tokenized_text[i] == '[MASK]':\n",
        "        # Create a copy of the tokenized text and replace the mask with a placeholder token\n",
        "        masked_tokenized_text = tokenized_text.copy()\n",
        "        masked_tokenized_text[i] = '[PREDICT]'\n",
        "        \n",
        "        # Convert the masked tokenized text to a tensor of token ids\n",
        "        masked_index = i\n",
        "        masked_index_tensor = torch.tensor([masked_index])\n",
        "        indexed_tokens = tokenizer.convert_tokens_to_ids(masked_tokenized_text)\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "        # Move the tokens tensor to the CPU\n",
        "        tokens_tensor = tokens_tensor.to(device)\n",
        "\n",
        "        # Generate predictions for the masked token using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tokens_tensor)\n",
        "            predictions = outputs[0][0, masked_index_tensor].topk(5)\n",
        "\n",
        "        # Convert the predicted token ids to tokens\n",
        "        predicted_token_ids = predictions.indices.tolist()\n",
        "        predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "\n",
        "        # Replace the placeholder token with the predicted token\n",
        "        tokenized_text[i] = predicted_tokens[0]\n",
        "\n",
        "# Convert the tokenized text to a string\n",
        "predicted_text = tokenizer.convert_tokens_to_string(tokenized_text)\n",
        "\n",
        "# Print the predicted text\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0rjr19q1YoC",
        "outputId": "8f2dcb2f-adb2-47bd-8d2b-549d466b4499"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le chat noir est sur le canapé . Le chat blanc est sur le tapis .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up input sentence(s) with masked tokens\n",
        "sentences = [\"I want to [MASK] a new car\", \"The [MASK] is blue\"]\n",
        "tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "masked_index = [sentence.index('[MASK]') for sentence in tokenized_sentences]\n",
        "\n",
        "# Generate predictions for masked tokens\n",
        "for i in range(len(sentences)):\n",
        "    tokenized_text = tokenized_sentences[i]\n",
        "    mask_pos = masked_index[i]\n",
        "    \n",
        "    # Replace masked token with [MASK] token\n",
        "    tokenized_text[mask_pos] = '[MASK]'\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    \n",
        "    # Predict probability distribution of next token\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0][0, mask_pos].topk(5).indices.tolist()\n",
        "\n",
        "    # Print predicted tokens\n",
        "    predicted_tokens = [tokenizer.convert_ids_to_tokens([prediction])[0] for prediction in predictions]\n",
        "    print(f\"For sentence '{sentences[i]}', the top 5 predicted tokens for the masked token are: {predicted_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz7vh7mQ2A9J",
        "outputId": "0093ff34-86d2-477c-e485-dbbd090e7db7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For sentence 'I want to [MASK] a new car', the top 5 predicted tokens for the masked token are: ['build', 'buy', 'make', 'create', 'construct']\n",
            "For sentence 'The [MASK] is blue', the top 5 predicted tokens for the masked token are: ['It', 'This', 'it', 'What', 'There']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"I want to build a car. But, I don't know [MASK] [MASK] [MASK] is the right way to do it.\"\n",
        "input_sentence += '.' # Add period to end of sentence\n",
        "first_sentence, second_sentence = input_sentence.split('.')\n",
        "tokenized_sentences = tokenizer.tokenize(first_sentence + '.' + ' [MASK] [MASK] [MASK]' + '.')\n",
        "mask_pos = tokenized_sentences.index('[MASK]')\n",
        "# Replace masked tokens with [MASK] token\n",
        "tokenized_sentences[mask_pos] = '[MASK]'\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentences)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Predict probability distribution of next sentence\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0].squeeze()\n",
        "\n",
        "# Print predicted sentence\n",
        "predicted_sentence = \" \".join(tokenizer.convert_ids_to_tokens(predictions.tolist()))\n",
        "print(f\"For input sentence '{input_sentence}', the predicted masked sentence is: {predicted_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "5xHJ507h3wQd",
        "outputId": "023c6657-25b9-4994-8fb3-006e1086dd83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-87a3e9150e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I want to build a car. But, I don't know [MASK] [MASK] [MASK] is the right way to do it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;31m# Add period to end of sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfirst_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' [MASK] [MASK] [MASK]'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmask_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[MASK]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}